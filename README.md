# CV Extractor Project

Extract structured information from CVs and resumes with AI precision.

![CV Extractor Interface](cv_extractor/web/static/interface1.png)


## ğŸ” Overview

This project extracts structured information from CVs/resumes using PDF processing techniques and LLM-powered text analysis. It features a comprehensive evaluation system to compare the performance of different language models.

## âœ¨ Features

- ğŸ“„ **PDF Text Extraction**: Extract text from digital PDFs with PyMuPDF
- ğŸ” **Advanced OCR**: Process scanned documents with Google's Gemini model
- ğŸ¤– **Multi-Model Support**: Compare extractions from multiple LLMs
- ğŸŒ **Web Interface**: Upload and process CVs through a browser
- ğŸ”Œ **API Access**: Programmatic access for integration
- ğŸ“Š **Detailed Analytics**: Compare model performance with metrics
- ğŸ’¾ **Structured Output**: Results formatted in consistent JSON
- ğŸ“š **Batch Processing**: Process multiple CVs in one go

## ğŸš€ Getting Started

### Docker Setup (Recommended)

```bash
# Build and start with Docker Compose
docker-compose up -d

# Or build the Docker image manually
docker build -t cv-extractor .

# Run the container (Windows PowerShell)
docker run -p 5000:5000 -v ${PWD}/data:/app/data -e "GOOGLE_API_KEY=your_key_here" -e "OPENROUTER_API_KEY=your_key_here" cv-extractor

# Run the container (Linux/Mac)
docker run -p 5000:5000 -v $(pwd)/data:/app/data -e "GOOGLE_API_KEY=your_key_here" -e "OPENROUTER_API_KEY=your_key_here" cv-extractor
```

### Local Setup

1. **Clone the repository**

```bash
git clone https://github.com/Hiba2222/cv_extractor_project.git
cd Resume_extractor_project
```

2. **Set up a virtual environment**

```bash
python -m venv venv
# On Windows
venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Configure environment variables**

```bash
cp .env.example .env
# Edit .env with your API keys and paths
```

> **Model Access Options:** 
> - Configure Ollama for local model inference (recommended for privacy)
> - Or set up an OpenRouter API key for cloud-based model access (alternative if Ollama setup is problematic)
> - System will automatically use OpenRouter if Ollama is unavailable

5. **Start the application**

```bash
# New recommended way
python bin/run_web.py

# Or use the legacy entry point (redirects to bin/)
python run_web.py
```

## ğŸ–¥ï¸ System Requirements

- **Operating System**: Windows 10+, macOS 10.15+, or Linux (Ubuntu 18.04+)
- **Python**: 3.9 or higher
- **RAM**: 12GB minimum, 16GB recommended
- **Storage**: 500MB for application, plus space for CV storage
- **Dependencies**:
  - Poppler (for PDF processing)
  - CUDA-compatible GPU (optional, for faster processing)

## ğŸ§  Models Supported

| Model        | Provider      | Type          | Best For              | Integration |
|--------------|---------------|---------------|------------------------|------------|
| Llama3       | Ollama/Meta   | Local/API     | Overall extraction     | Direct/OpenRouter |
| Mistral      | Mistral AI    | API           | Education & experience | OpenRouter |
| Phi-3        | Microsoft     | API           | Basic fields           | OpenRouter |
| GPT-4        | OpenAI        | API           | Complex documents      | Direct     |
| Gemini 1.5 Flash | Google    | API           | OCR & image extraction | Direct     |

> **Note:** For users experiencing issues with Ollama, the system provides automatic fallback to OpenRouter API for accessing models like Llama3, ensuring uninterrupted service.

## âš™ï¸ How It Works

1. **Document Processing**:
   - PDF text extraction using PyMuPDF
   - Image-based PDFs processed with OCR via Gemini 1.5 Flash
   - Text cleaning and normalization

2. **Information Extraction**:
   - Structured prompting to LLMs
   - Field-specific extraction patterns
   - Fallback mechanisms between models
   - Automatic switching between Ollama (local) and OpenRouter (cloud) as needed

3. **Result Processing**:
   - JSON formatting and validation
   - Confidence scoring for extracted fields
   - Structure normalization

4. **Evaluation Pipeline**:
   - Ground truth comparison
   - Field-specific metrics
   - Cross-model performance evaluation

## ğŸ“Š Model Performance

Our evaluation system compares model performance across different CV fields:

| Model   | Name  | Email | Phone | Skills | Education | Experience | Overall |
|---------|-------|-------|-------|--------|-----------|------------|---------|
| Llama3  | 1.00  | 1.00  | 0.98  | 0.74   | 0.92      | 0.47       | 0.81    |
| Mistral | 0.75  | 0.74  | 0.75  | 0.42   | 0.74      | 0.36       | 0.59    |
| Phi     | 0.38  | 0.74  | 0.75  | 0.38   | 0.74      | 0.38       | 0.52    |

*Note: Run `python bin/run_evaluation.py` to generate performance visualizations and detailed reports.*

## ğŸ’¡ Usage Instructions

### Using Launcher Scripts (Recommended)

The project includes launcher scripts for easy startup:

**Windows Users:**
```bash
# Double-click run.bat or run from PowerShell/Command Prompt
.\run.bat
```

**Linux/Mac Users:**
```bash
# Make the script executable first
chmod +x run.sh
# Run the script
./run.sh
```

Both scripts provide an interactive menu to:
1. Start the application with Docker (recommended)
2. Start with Python directly
3. Stop running Docker containers
4. Exit

### Web Interface

1. Access the web interface at http://localhost:5000
2. Upload a CV/resume in PDF format
3. Select an AI model for processing
4. View structured results and download as JSON

### API Usage

```bash
# Basic usage
curl -X POST -F "pdf_file=@/path/to/your/cv.pdf" -F "models=llama3" http://localhost:5000/api/extract

# Multiple models
curl -X POST -F "pdf_file=@/path/to/your/cv.pdf" -F "models=llama3,mistral,phi" http://localhost:5000/api/extract

# With specific parameters
curl -X POST -F "pdf_file=@/path/to/your/cv.pdf" -F "models=llama3" -F "confidence_threshold=0.7" http://localhost:5000/api/extract
```

### Command-line Processing

Run the complete pipeline:

```bash
python bin/pipeline.py --input data/input --output data/output --results data/results --models llama3,mistral,phi
```

Or use individual modules:

```bash
# Extract text from PDFs
python -m cv_extractor.pdf.extractor

# Process text with LLMs
python -m cv_extractor.llm.processor --input input.txt --output output.json
```

## ğŸ” Evaluation System

### Running an Evaluation

To evaluate model performance:

```bash
python bin/run_evaluation.py
```

This script will:
1. Read and normalize all ground truth files
2. Process model results
3. Generate test results for missing model-CV combinations
4. Calculate performance metrics
5. Create visualizations and reports

### Adding Custom Evaluation Data

1. **Create ground truth files**:
   - Add JSON files to `data/ground_truth/` named like `gt1.json`, `gt2.json`, etc.
   - Follow the existing format with fields like "Name", "Email", "Phone", "Skills", etc.

2. **Add model results**:
   - Process CVs with your models
   - Save results to `data/evaluation/model_results/{model_name}/` directory
   - Follow the naming convention: `cv{number}_result.json`

## ğŸ“ Directory Structure

```
Resume_extractor_project/
â”œâ”€â”€ cv_extractor/             # Main Python package
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ config.py             # Centralized configuration
â”‚   â”œâ”€â”€ pdf/                  # PDF processing module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ extractor.py      # PDF text extraction
â”‚   â”œâ”€â”€ llm/                  # LLM processing module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ processor.py      # LLM integration
â”‚   â”œâ”€â”€ web/                  # Web interface module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py            # Flask web application
â”‚   â”‚   â”œâ”€â”€ static/           # Static CSS, JS, and images
â”‚   â”‚   â”‚   â”œâ”€â”€ style.css
â”‚   â”‚   â”‚   â”œâ”€â”€ scripts.js
â”‚   â”‚   â”‚   â””â”€â”€ interface1.png
â”‚   â”‚   â””â”€â”€ templates/        # Flask HTML templates
â”‚   â”‚       â”œâ”€â”€ base.html
â”‚   â”‚       â”œâ”€â”€ index.html
â”‚   â”‚       â”œâ”€â”€ result.html
â”‚   â”‚       â”œâ”€â”€ error.html
â”‚   â”‚       â””â”€â”€ macros.html
â”‚   â””â”€â”€ evaluation/           # Evaluation module
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ core.py           # Evaluation framework
â”‚       â””â”€â”€ runner.py         # Evaluation pipeline
â”œâ”€â”€ bin/                      # Entry point scripts
â”‚   â”œâ”€â”€ run_web.py            # Web application launcher
â”‚   â”œâ”€â”€ pipeline.py           # Processing pipeline
â”‚   â””â”€â”€ run_evaluation.py    # Evaluation runner
â”œâ”€â”€ data/                     # Data directories
â”‚   â”œâ”€â”€ input/                # CV PDFs to process
â”‚   â”œâ”€â”€ output/               # Extracted text files
â”‚   â”œâ”€â”€ results/              # Structured JSON results
â”‚   â”œâ”€â”€ uploads/              # Web upload temporary storage
â”‚   â”œâ”€â”€ ground_truth/         # Manual annotations for evaluation
â”‚   â”‚   â”œâ”€â”€ gt1.json
â”‚   â”‚   â”œâ”€â”€ gt2.json
â”‚   â”‚   â”œâ”€â”€ gt3.json
â”‚   â”‚   â”œâ”€â”€ gt5.json
â”‚   â”‚   â””â”€â”€ gt6.json
â”‚   â””â”€â”€ evaluation/           # Processed evaluation data
â”‚       â”œâ”€â”€ combined_ground_truth.json
â”‚       â””â”€â”€ model_results/    # Results by model
â”‚           â”œâ”€â”€ llama3/
â”‚           â”œâ”€â”€ mistral/
â”‚           â””â”€â”€ phi/
â”œâ”€â”€ pipeline.py               # Legacy entry point (redirects to bin/)
â”œâ”€â”€ run_web.py                # Legacy entry point (redirects to bin/)
â”œâ”€â”€ run.sh                    # Launcher script for Linux/Mac
â”œâ”€â”€ run.bat                   # Launcher script for Windows
â”œâ”€â”€ Dockerfile                # Container definition
â”œâ”€â”€ docker-compose.yml        # Docker orchestration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Example environment variables
â”œâ”€â”€ .env                      # Environment variables (not in git)
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ REORGANIZATION_SUMMARY.md # Project reorganization notes
â””â”€â”€ README.md                 # Project documentation
```


## ğŸ³ Dockerization

The project includes Docker support for easy deployment and consistent runtime environment.

### Building and Running with Docker Compose

The simplest way to run the application:

```bash
# Build and start the container
docker-compose up -d

# View logs
docker-compose logs -f

# Stop the container
docker-compose down
```

### Building and Running with Docker

If you prefer using Docker directly:

```bash
# Build the Docker image
docker build -t cv-extractor .

# Run the container (Windows PowerShell)
docker run -p 5000:5000 -v ${PWD}/data:/app/data -e "GOOGLE_API_KEY=your_key_here" -e "OPENROUTER_API_KEY=your_key_here" cv-extractor

# Run the container (Linux/Mac)
docker run -p 5000:5000 -v $(pwd)/data:/app/data -e "GOOGLE_API_KEY=your_key_here" -e "OPENROUTER_API_KEY=your_key_here" cv-extractor
```

### Container Details

- **Base image**: Python 3.9 (slim)
- **Exposed port**: 5000
- **Mounted volumes**:
  - `./data:/app/data`: Persists all data files (input, output, results, uploads)
- **Dependencies**: Includes Poppler for PDF processing and other required system libraries

### Environment Variables

Environment variables can be set in the `.env` file or passed directly to the container:

**Required:**
- `GOOGLE_API_KEY`: Google Gemini API key for OCR processing
- `OPENROUTER_API_KEY`: OpenRouter API key for LLM access

**Optional:**
- `DEFAULT_MODELS`: Comma-separated list of models (default: `llama3,mistral,phi`)
- `FLASK_PORT`: Web server port (default: `5000`)
- `FLASK_ENV`: Environment mode (default: `development`)
- `OCR_ENABLED`: Enable OCR for scanned PDFs (default: `true`)
- `LOG_LEVEL`: Logging level (default: `INFO`)

**Example:**
```bash
docker run -p 5000:5000 \
  -v $(pwd)/data:/app/data \
  -e "GOOGLE_API_KEY=your_google_key" \
  -e "OPENROUTER_API_KEY=your_openrouter_key" \
  -e "DEFAULT_MODELS=llama3,mistral" \
  cv-extractor
```

## ğŸ“„ License

[MIT License](LICENSE)

## ğŸ™ Acknowledgements

- Google's Gemini 1.5 Flash API for OCR capabilities
- PyMuPDF for PDF processing
- Ollama for local LLM inference 